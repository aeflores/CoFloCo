MAYBE

Initial complexity problem:
1:	T:
		(1, 1)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize)
		(?, 1)    eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb12_in(v_20, v_21, v_fsize.0, v_lsize)
		(?, 1)    eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, 0, v_lsize)
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb8_in(v_20, v_21, v_fsize.0, v_lsize) [ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb8_in(v_20, v_21, v_fsize.0, v_lsize) [ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb11_in(v_20, v_21, v_fsize.0, v_lsize) [ v_fsize.0 >= v_20 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb11_in(v_20, v_21, v_fsize.0, v_lsize) [ v_fsize.0 = v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb8_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb12_in(v_20, v_21, v_fsize.0, v_lsize)
		(?, 1)    eval_gx_bits_cache_alloc_bb11_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb12_in(v_20, v_21, v_fsize.0, v_lsize)
		(?, 1)    eval_gx_bits_cache_alloc_bb12_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_stop(v_20, v_21, v_fsize.0, v_lsize)
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	0

Repeatedly removing leaves of the complexity graph in problem 1 produces the following problem:
2:	T:
		(1, 1)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize)
		(?, 1)    eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, 0, v_lsize)
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

Repeatedly propagating knowledge in problem 2 produces the following problem:
3:	T:
		(1, 1)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize)
		(1, 1)    eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, 0, v_lsize)
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

Applied AI with 'oct' on problem 3 to obtain the following invariants:
  For symbol eval_gx_bits_cache_alloc_bb3_in: X_2 - X_4 >= 0 /\ X_1 - X_4 - 8 >= 0 /\ -X_2 + X_4 >= 0 /\ -X_1 + X_4 + 8 >= 0 /\ X_1 - X_2 - 8 >= 0 /\ -X_1 + X_2 + 8 >= 0


This yielded the following problem:
4:	T:
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(1, 1)    eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, 0, v_lsize)
		(1, 1)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize)
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize) with all transitions in problem 4, the following new transition is obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, 0, v_lsize)
We thus obtain the following problem:
5:	T:
		(1, 2)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, 0, v_lsize)
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(1, 1)    eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, 0, v_lsize)
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

Testing for reachability in the complexity graph removes the following transition from problem 5:
	eval_gx_bits_cache_alloc_bb0_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, 0, v_lsize)
We thus obtain the following problem:
6:	T:
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
		(1, 2)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, 0, v_lsize)
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, 0, v_lsize) with all transitions in problem 6, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
We thus obtain the following problem:
7:	T:
		(1, 3)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize ]
		(1, 3)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize ] with all transitions in problem 7, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
We thus obtain the following problem:
8:	T:
		(1, 4)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize ]
		(1, 4)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize ] with all transitions in problem 8, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
We thus obtain the following problem:
9:	T:
		(1, 5)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize ]
		(1, 5)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize ] with all transitions in problem 9, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
We thus obtain the following problem:
10:	T:
		(1, 6)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize ]
		(1, 6)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
		(1, 5)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize ] with all transitions in problem 10, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' > v_lsize ]
We thus obtain the following problem:
11:	T:
		(1, 7)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize ]
		(1, 7)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' > v_lsize ]
		(1, 6)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
		(1, 5)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize ] with all transitions in problem 11, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' > v_lsize ]
We thus obtain the following problem:
12:	T:
		(1, 8)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize ]
		(1, 8)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' > v_lsize ]
		(1, 7)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' > v_lsize ]
		(1, 6)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
		(1, 5)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize ] with all transitions in problem 12, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' > v_lsize ]
We thus obtain the following problem:
13:	T:
		(1, 9)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize ]
		(1, 9)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' > v_lsize ]
		(1, 8)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' > v_lsize ]
		(1, 7)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' > v_lsize ]
		(1, 6)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
		(1, 5)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)    eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize ] with all transitions in problem 13, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' > v_lsize ]
We thus obtain the following problem:
14:	T:
		(1, 10)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize ]
		(1, 10)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' > v_lsize ]
		(1, 9)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' > v_lsize ]
		(1, 8)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' > v_lsize ]
		(1, 7)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' > v_lsize ]
		(1, 6)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
		(1, 5)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize ] with all transitions in problem 14, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' > v_lsize ]
We thus obtain the following problem:
15:	T:
		(1, 11)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize ]
		(1, 11)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' > v_lsize ]
		(1, 10)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' > v_lsize ]
		(1, 9)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' > v_lsize ]
		(1, 8)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' > v_lsize ]
		(1, 7)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' > v_lsize ]
		(1, 6)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
		(1, 5)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize ] with all transitions in problem 15, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' > v_lsize ]
We thus obtain the following problem:
16:	T:
		(1, 12)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize ]
		(1, 12)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' > v_lsize ]
		(1, 11)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' > v_lsize ]
		(1, 10)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' > v_lsize ]
		(1, 9)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' > v_lsize ]
		(1, 8)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' > v_lsize ]
		(1, 7)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' > v_lsize ]
		(1, 6)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
		(1, 5)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize ] with all transitions in problem 16, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' > v_lsize ]
We thus obtain the following problem:
17:	T:
		(1, 13)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize ]
		(1, 13)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' > v_lsize ]
		(1, 12)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' > v_lsize ]
		(1, 11)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' > v_lsize ]
		(1, 10)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' > v_lsize ]
		(1, 9)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' > v_lsize ]
		(1, 8)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' > v_lsize ]
		(1, 7)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' > v_lsize ]
		(1, 6)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
		(1, 5)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize ] with all transitions in problem 17, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' > v_lsize ]
We thus obtain the following problem:
18:	T:
		(1, 14)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize ]
		(1, 14)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' > v_lsize ]
		(1, 13)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' > v_lsize ]
		(1, 12)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' > v_lsize ]
		(1, 11)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' > v_lsize ]
		(1, 10)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' > v_lsize ]
		(1, 9)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' > v_lsize ]
		(1, 8)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' > v_lsize ]
		(1, 7)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' > v_lsize ]
		(1, 6)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
		(1, 5)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize ] with all transitions in problem 18, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' > v_lsize ]
We thus obtain the following problem:
19:	T:
		(1, 15)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize ]
		(1, 15)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' > v_lsize ]
		(1, 14)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' > v_lsize ]
		(1, 13)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' > v_lsize ]
		(1, 12)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' > v_lsize ]
		(1, 11)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' > v_lsize ]
		(1, 10)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' > v_lsize ]
		(1, 9)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' > v_lsize ]
		(1, 8)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' > v_lsize ]
		(1, 7)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' > v_lsize ]
		(1, 6)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
		(1, 5)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

By chaining the transition eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize ] with all transitions in problem 19, the following new transitions are obtained:
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' + nondef.4''''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' < v_lsize ]
	eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' + nondef.4''''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' > v_lsize ]
We thus obtain the following problem:
20:	T:
		(1, 16)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' + nondef.4''''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' < v_lsize ]
		(1, 16)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' + nondef.4''''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''' > v_lsize ]
		(1, 15)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' + nondef.4'''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''' > v_lsize ]
		(1, 14)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' + nondef.4''''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''' > v_lsize ]
		(1, 13)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' + nondef.4'''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''' > v_lsize ]
		(1, 12)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' + nondef.4''''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''' > v_lsize ]
		(1, 11)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' + nondef.4'''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''' > v_lsize ]
		(1, 10)    eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' + nondef.4''''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''' > v_lsize ]
		(1, 9)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' + nondef.4'''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''' > v_lsize ]
		(1, 8)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' + nondef.4''''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''' > v_lsize ]
		(1, 7)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''' + nondef.4'''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' + nondef.4''' > v_lsize ]
		(1, 6)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'' + nondef.4''', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' < v_lsize /\ nondef.4 + nondef.4' + nondef.4'' < v_lsize + 8 /\ nondef.4 + nondef.4' + nondef.4'' > v_lsize ]
		(1, 5)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4' + nondef.4'', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 < v_lsize /\ nondef.4 + nondef.4' < v_lsize + 8 /\ nondef.4 + nondef.4' > v_lsize ]
		(1, 4)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4 + nondef.4', v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 < v_lsize /\ nondef.4 < v_lsize + 8 /\ nondef.4 > v_lsize ]
		(1, 3)     eval_gx_bits_cache_alloc_start(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_lsize + 8, v_lsize, nondef.4, v_lsize) [ 0 >= 0 /\ 0 < v_lsize + 8 /\ 0 > v_lsize ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 < v_21 ]
		(?, 1)     eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0, v_lsize) -> eval_gx_bits_cache_alloc_bb3_in(v_20, v_21, v_fsize.0 + nondef.4, v_lsize) [ v_21 - v_lsize >= 0 /\ v_20 - v_lsize - 8 >= 0 /\ -v_21 + v_lsize >= 0 /\ -v_20 + v_lsize + 8 >= 0 /\ v_20 - v_21 - 8 >= 0 /\ -v_20 + v_21 + 8 >= 0 /\ v_fsize.0 < v_20 /\ v_fsize.0 > v_21 ]
	start location:	eval_gx_bits_cache_alloc_start
	leaf cost:	8

Complexity upper bound ?

Time: 1.695 sec (SMT: 1.069 sec)
